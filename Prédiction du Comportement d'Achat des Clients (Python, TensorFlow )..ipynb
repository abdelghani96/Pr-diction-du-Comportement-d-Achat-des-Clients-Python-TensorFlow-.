{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de rentabilisation des livres audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description du Projet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données d'une application Audiobook vous sont fournies. Logiquement, il se rapporte aux versions audio des livres SEULEMENT. Chaque client dans la base de données a effectué un achat au moins une fois, c'est pourquoi il est dans la base de données. Nous voulons créer un algorithme d'apprentissage machine basé sur nos données disponibles qui permet de prédire si un client achètera à nouveau un livre audio ou pas.\n",
    "<br /> La tâche est simple : créer un algorithme d'apprentissage machine, qui est capable de prédire si un client va acheter à nouveau.\n",
    "<br /> Il s'agit d'un problème de **classification** avec deux classes : ne veut pas acheter et achète, représenté par 0 et 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'utilité du projet du coté Marketing (Prenons \"AudioBook\" comme exemple d'une socièté)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée principale est que si un client a une faible probabilité de revenir, il n'y a aucune raison de dépenser de l'argent en publicité auprès de lui. Si nous pouvons concentrer nos efforts UNIQUEMENT sur les clients qui sont susceptibles de se reconvertir, nous pouvons réaliser d'importantes économies. De plus, ce modèle permet d'identifier les paramètres les plus importants pour qu'un client revienne. L'identification de nouveaux clients crée de la valeur et des opportunités de croissance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description des variables (features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Vous avez un fichier.csv qui résume les données. Il y a plusieurs variables : <br /> **Customer ID** : l'identifiant de chaque client <br /> **Longueur totale du livre** : somme des minutes de tous les achats <br /> **Longueur moyenne du livre** : longueur moyenne en minutes de tous les achats <br /> **Prix payé_global** : somme de tous les achats <br /> **Moyenne des prix payés** : moyenne de tous les achats<br /> **La revue** : (variable booléenne) si le client a quitté un examen <br /> **Evaluation du produit sur 10** : si le client a quitté un examen, examen sur 10 <br /> **Total de minutes écoutées** : nombre total des minutes écoutés par le client  <br /> **Complétion** = Total de minutes écoutées/Longueur totale du livre (variable numérique entre 0 et 1)<br /> **support technique** : nombre de demandes de support technique ,tout, du mot de passe oublié à l'assistance pour l'utilisation de l'application  <br />  **la date de la dernière visite moins la date d'achat (en jours)** : C'est une variable importante qui sert à préciser si un client prend le temps lors de son achat (si 0 alors le client n'a pas joué l'audio qu'il a acheté.<br /> Ce sont les entrées (à l'exclusion de l'ID client, car il est totalement arbitraire. C'est plus un nom qu'un numéro).<br /> **Targets(Les cibles)** : sont une variable booléenne (0 ou 1). Nous prenons une période de 2 ans dans nos entrées, et les 6 prochains mois comme objectifs. Ainsi, en fait, nous prédisons si : sur la base des 2 dernières années d'activité et d'engagement, un client va se convertir dans les 6 prochains mois. 6 mois, c'est un délai raisonnable. S'ils ne se convertissent pas après 6 mois, il y a de fortes chances qu'ils soient allés chez un concurrent ou qu'ils n'aient pas aimé la façon dont Audiobook digère l'information.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "## Nous utiliserons la bibliothèque de prétraitement sklearn, car il sera plus facile de standardiser les données.\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Charger les données\n",
    "raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')\n",
    "\n",
    "\n",
    "# Les entrées sont toutes les colonnes du csv, à l'exception de la première [ :,0].\n",
    "# (qui n'est que l'identification arbitraire des clients qui ne portent aucune information utile),\n",
    "# et le dernier [ :,-1] (qui est notre cible)\n",
    "\n",
    "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
    "\n",
    "# Les cibles sont dans la dernière colonne.\n",
    "targets_all = raw_csv_data[:,-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blancer Les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comptez combien de cibles sont 1 (ce qui signifie que le client a converti).\n",
    "num_one_targets=int(np.sum(targets_all))\n",
    "\n",
    "# Définir un compteur pour les cibles qui sont à 0 (ce qui signifie que le client n'a pas converti)\n",
    "zero_targets_counter = 0\n",
    "\n",
    "# Nous voulons créer un ensemble de données \"équilibré\", nous devrons donc supprimer certaines paires entrées/cibles.\n",
    "# Déclarez une variable qui le fera :\n",
    "indices_to_remove = []\n",
    "\n",
    "# Comptez le nombre de cibles qui sont à 0. \n",
    "# Une fois qu'il y a autant de 0 que de 1, marquez les entrées où la cible est 0.\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "# Créez deux nouvelles variables, une qui contiendra les entrées et une autre qui contiendra les cibles.\n",
    "# Nous supprimons tous les index que nous avons marqués \"supprimer\" dans la boucle ci-dessus.\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardiser les entrées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C'est le seul endroit où nous utilisons la fonctionnalité sklearn. Nous tirerons parti de ses capacités de prétraitement\n",
    "# C'est une simple ligne de code, qui standardise les entrées, comme nous l'avons expliqué dans l'une des conférences.\n",
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mélanger les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quand les données ont été collectées, elles ont été classées par date\n",
    "# Mélangez les index des données, de sorte que les données ne sont pas organisées de quelque façon que ce soit lorsque nous les alimentons.\n",
    "# Puisque nous allons mettre en lots, nous voulons que les données soient réparties aussi aléatoirement que possible\n",
    "\n",
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "# Utilisez les index mélangés pour mélanger les entrées et les cibles.\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diviser les données en Entrainement, Validation et Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1789.0 3579 0.49986029617211514\n",
      "211.0 447 0.4720357941834452\n",
      "237.0 448 0.5290178571428571\n"
     ]
    }
   ],
   "source": [
    "# Comptez le nombre total d'échantillons\n",
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "# Comptez les échantillons dans chaque sous-ensemble, en supposant que nous voulons une distribution 80-10-10 de l'entrainement', de la validation et du test.\n",
    "# Naturellement, les nombres sont entiers.\n",
    "train_samples_count = int(0.8 * samples_count)\n",
    "validation_samples_count = int(0.1 * samples_count)\n",
    "\n",
    "# L'ensemble de données'test' contient toutes les données restantes.\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "# Créer des variables qui enregistrent les entrées et les cibles pour l'entrainement\n",
    "# Dans notre ensemble de données mélangées, ce sont les premières observations \"train_samples_count\".\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "# Créer des variables qui enregistrent les entrées et les cibles à valider.\n",
    "# Ce sont les prochaines observations \"validation_samples_count\", suivant le \"train_samples_count\" que nous avons déjà assigné\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "# Créer des variables qui enregistrent les entrées et les cibles pour le test.\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
    "\n",
    "\n",
    "#Retourner le nombre de cibles qui sont 1s, le nombre total d'échantillons et la proportion pour l'entraînement, la validation et le test.\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegardez les trois ensembles de données dans *.npz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('Audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créez une classe qui regroupera les données\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créez une classe qui fera le batching pour l'algorithme\n",
    "\n",
    "class Audiobooks_Data_Reader():\n",
    "    # L'ensemble de données est un arugment obligatoire, alors que la taille du lot est facultative.\n",
    "    def __init__(self, dataset, batch_size = None):\n",
    "    \n",
    "        # L'ensemble de données qui charge en est un de \"train\", \"validation\", \"test\".\n",
    "        # Par exemple, si j'appelle cette classe avec x('train',5), elle chargera 'Audiobooks_data_train.npz' avec une taille de lot de 5.\n",
    "\n",
    "        npz = np.load('Audiobooks_data_{0}.npz'.format(dataset))\n",
    "        \n",
    "        # Deux variables qui prennent les valeurs des entrées et des cibles. Les entrées sont des flotteurs, les cibles sont des entiers\n",
    "        self.inputs, self.targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
    "        \n",
    "        # Compte le numéro de lot, compte tenu de la taille que vous lui donnerez plus tard\n",
    "        # Si la taille du lot est Aucune, nous validons ou testons, donc nous voulons prendre les données dans un seul lot.\n",
    "        if batch_size is None:\n",
    "            self.batch_size = self.inputs.shape[0]\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        self.curr_batch = 0\n",
    "        self.batch_count = self.inputs.shape[0] // self.batch_size\n",
    "    \n",
    "    # la méthode pour charger le lot suivant\n",
    "    def __next__(self):\n",
    "        if self.curr_batch >= self.batch_count:\n",
    "            self.curr_batch = 0\n",
    "            raise StopIteration()\n",
    "            \n",
    "        # Vous coupez l'ensemble de données par lots et la fonction \"next\" les charge l'un après l'autre.\n",
    "        batch_slice = slice(self.curr_batch * self.batch_size, (self.curr_batch + 1) * self.batch_size)\n",
    "        inputs_batch = self.inputs[batch_slice]\n",
    "        targets_batch = self.targets[batch_slice]\n",
    "        self.curr_batch += 1\n",
    "        \n",
    "        # One-hot encoder les cibles.\n",
    "        classes_num = 2\n",
    "        targets_one_hot = np.zeros((targets_batch.shape[0], classes_num))\n",
    "        targets_one_hot[range(targets_batch.shape[0]), targets_batch] = 1\n",
    "        \n",
    "        # La fonction retournera le lot d'entrées et les cibles codées à un chaud.\n",
    "\n",
    "        return inputs_batch, targets_one_hot\n",
    "    \n",
    "        \n",
    "    # Une méthode nécessaire pour l'itération sur les lots, car nous allons les mettre en boucle\n",
    "    # Cela indique à Python que la classe que nous définissons est itérable, c'est-à-dire que nous pouvons l'utiliser comme :\n",
    "    # pour l'entrée, la sortie dans les données : \n",
    "        # do things\n",
    "    # Un itérateur en Python est une classe avec une méthode __next__ qui définit exactement comment itérer à travers ses objets\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créer l'algorithme d'apprentissage machine\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La Configuration des hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chhimy\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# La taille de l'entrée dépend du nombre de variables d'entrée. Nous en avons 10\n",
    "input_size = 10\n",
    "# Output size is 2, as we one-hot encoded the targets.\n",
    "output_size = 2\n",
    "# Choisissez une taille de couche cachée\n",
    "hidden_layer_size = 100\n",
    "\n",
    "# Réinitialisez le graphique par défaut, afin de pouvoir modifier les hyperparamètres, puis réexécuter le code.\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créez les espaces réservés\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.float32,[None, input_size])\n",
    "targets = tf.placeholder(tf.int32,[None, output_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### description du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons créer un réseau avec 2 calques cachés\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poids et biais pour la première combinaison linéaire entre les entrées et la première couche cachée.\n",
    "# la méthode get_variable pour utiliser l'initialisateur par défaut de TensorFlow qui est Xavier.\n",
    "weights_1 = tf.get_variable(\"weights_1\",[input_size, hidden_layer_size])\n",
    "biases_1 = tf.get_variable(\"biases_1\",[hidden_layer_size])\n",
    "# pour la sortie nous utilisons la fonction d'activation ReLu \n",
    "outputs_1 = tf.nn.relu(tf.matmul(inputs, weights_1) + biases_1)\n",
    "\n",
    "weights_2 = tf.get_variable(\"weights_2\",[hidden_layer_size, hidden_layer_size])\n",
    "biases_2 = tf.get_variable(\"biases_2\",[hidden_layer_size])\n",
    "# pour la sortie nous utilisons la fonction d'activation Sigmoid\n",
    "outputs_2 = tf.nn.sigmoid(tf.matmul(outputs_1, weights_2) + biases_2)\n",
    "\n",
    "weights_3 = tf.get_variable(\"weights_3\",[hidden_layer_size, output_size])\n",
    "biases_3 = tf.get_variable(\"biases_3\",[output_size])\n",
    "# Nous allons incorporer l'activation softmax dans la perte, \n",
    "outputs = tf.matmul(outputs_2, weights_3) + biases_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the loss function (la fonction de perte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-d471591fd4f6>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nous allons incorporer l'activation softmax dans la perte, \n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=targets)\n",
    "mean_loss = tf.reduce_mean(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La mésure de la prècision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la précision est le facteur qui détermine la puissance de notre modèle puisque nous allons la calculer sur le jeu de données d'entrainement (Train) et tester sa performance sur le jeu de données de test(Test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir un 0 ou un 1 pour chaque entrée indiquant si elle fournit la bonne réponse.\n",
    "out_equals_target = tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'Etape d'Optimisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize with Adam\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(mean_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lancement d'une session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pour démarrer le programme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créez une session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Initialiser les variables\n",
    "initializer = tf.global_variables_initializer()\n",
    "sess.run(initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# l'entrainement du modèle (Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoque1. Perte entraînement: 0.626. Perte de validation: 0.543. Précision de validation: 69.35%\n",
      "Epoque2. Perte entraînement: 0.488. Perte de validation: 0.455. Précision de validation: 78.75%\n",
      "Epoque3. Perte entraînement: 0.428. Perte de validation: 0.414. Précision de validation: 79.87%\n",
      "Epoque4. Perte entraînement: 0.397. Perte de validation: 0.391. Précision de validation: 80.76%\n",
      "Epoque5. Perte entraînement: 0.379. Perte de validation: 0.377. Précision de validation: 79.64%\n",
      "Epoque6. Perte entraînement: 0.367. Perte de validation: 0.367. Précision de validation: 79.87%\n",
      "Epoque7. Perte entraînement: 0.358. Perte de validation: 0.361. Précision de validation: 80.54%\n",
      "Epoque8. Perte entraînement: 0.352. Perte de validation: 0.356. Précision de validation: 81.21%\n",
      "Epoque9. Perte entraînement: 0.347. Perte de validation: 0.352. Précision de validation: 81.66%\n",
      "Epoque10. Perte entraînement: 0.343. Perte de validation: 0.348. Précision de validation: 81.88%\n",
      "Epoque11. Perte entraînement: 0.340. Perte de validation: 0.346. Précision de validation: 82.10%\n",
      "Epoque12. Perte entraînement: 0.337. Perte de validation: 0.344. Précision de validation: 82.10%\n",
      "Epoque13. Perte entraînement: 0.335. Perte de validation: 0.342. Précision de validation: 82.10%\n",
      "Epoque14. Perte entraînement: 0.333. Perte de validation: 0.340. Précision de validation: 82.10%\n",
      "Epoque15. Perte entraînement: 0.331. Perte de validation: 0.339. Précision de validation: 82.55%\n",
      "Epoque16. Perte entraînement: 0.330. Perte de validation: 0.337. Précision de validation: 82.33%\n",
      "Epoque17. Perte entraînement: 0.328. Perte de validation: 0.336. Précision de validation: 82.10%\n",
      "Epoque18. Perte entraînement: 0.327. Perte de validation: 0.334. Précision de validation: 82.10%\n",
      "Epoque19. Perte entraînement: 0.326. Perte de validation: 0.333. Précision de validation: 82.10%\n",
      "Epoque20. Perte entraînement: 0.325. Perte de validation: 0.332. Précision de validation: 82.10%\n",
      "Epoque21. Perte entraînement: 0.324. Perte de validation: 0.331. Précision de validation: 82.33%\n",
      "Epoque22. Perte entraînement: 0.323. Perte de validation: 0.330. Précision de validation: 82.33%\n",
      "Epoque23. Perte entraînement: 0.322. Perte de validation: 0.329. Précision de validation: 82.33%\n",
      "Epoque24. Perte entraînement: 0.321. Perte de validation: 0.329. Précision de validation: 82.55%\n",
      "Epoque25. Perte entraînement: 0.321. Perte de validation: 0.328. Précision de validation: 82.55%\n",
      "Epoque26. Perte entraînement: 0.320. Perte de validation: 0.327. Précision de validation: 82.55%\n",
      "Epoque27. Perte entraînement: 0.319. Perte de validation: 0.326. Précision de validation: 82.33%\n",
      "Epoque28. Perte entraînement: 0.319. Perte de validation: 0.326. Précision de validation: 82.33%\n",
      "Epoque29. Perte entraînement: 0.318. Perte de validation: 0.325. Précision de validation: 82.33%\n",
      "Epoque30. Perte entraînement: 0.318. Perte de validation: 0.324. Précision de validation: 82.33%\n",
      "Epoque31. Perte entraînement: 0.317. Perte de validation: 0.324. Précision de validation: 82.10%\n",
      "Epoque32. Perte entraînement: 0.317. Perte de validation: 0.323. Précision de validation: 82.10%\n",
      "Epoque33. Perte entraînement: 0.316. Perte de validation: 0.323. Précision de validation: 82.10%\n",
      "Epoque34. Perte entraînement: 0.316. Perte de validation: 0.323. Précision de validation: 82.33%\n",
      "Epoque35. Perte entraînement: 0.315. Perte de validation: 0.322. Précision de validation: 82.33%\n",
      "Epoque36. Perte entraînement: 0.315. Perte de validation: 0.322. Précision de validation: 82.10%\n",
      "Epoque37. Perte entraînement: 0.315. Perte de validation: 0.321. Précision de validation: 82.10%\n",
      "Epoque38. Perte entraînement: 0.314. Perte de validation: 0.321. Précision de validation: 82.10%\n",
      "Epoque39. Perte entraînement: 0.314. Perte de validation: 0.320. Précision de validation: 82.10%\n",
      "Epoque40. Perte entraînement: 0.314. Perte de validation: 0.320. Précision de validation: 81.88%\n",
      "Epoque41. Perte entraînement: 0.313. Perte de validation: 0.320. Précision de validation: 81.88%\n",
      "Epoque42. Perte entraînement: 0.313. Perte de validation: 0.319. Précision de validation: 81.88%\n",
      "Epoque43. Perte entraînement: 0.313. Perte de validation: 0.319. Précision de validation: 81.88%\n",
      "Epoque44. Perte entraînement: 0.312. Perte de validation: 0.319. Précision de validation: 81.21%\n",
      "Epoque45. Perte entraînement: 0.312. Perte de validation: 0.318. Précision de validation: 81.21%\n",
      "Epoque46. Perte entraînement: 0.312. Perte de validation: 0.318. Précision de validation: 81.21%\n",
      "Epoque47. Perte entraînement: 0.312. Perte de validation: 0.318. Précision de validation: 81.21%\n",
      "Epoque48. Perte entraînement: 0.311. Perte de validation: 0.318. Précision de validation: 81.66%\n",
      "Epoque49. Perte entraînement: 0.311. Perte de validation: 0.318. Précision de validation: 81.66%\n",
      "Epoque50. Perte entraînement: 0.311. Perte de validation: 0.317. Précision de validation: 81.66%\n",
      "Epoque51. Perte entraînement: 0.311. Perte de validation: 0.317. Précision de validation: 81.88%\n",
      "Epoque52. Perte entraînement: 0.310. Perte de validation: 0.317. Précision de validation: 82.10%\n",
      "Epoque53. Perte entraînement: 0.310. Perte de validation: 0.317. Précision de validation: 82.10%\n",
      "Epoque54. Perte entraînement: 0.310. Perte de validation: 0.316. Précision de validation: 82.55%\n",
      "Epoque55. Perte entraînement: 0.310. Perte de validation: 0.316. Précision de validation: 82.77%\n",
      "Epoque56. Perte entraînement: 0.310. Perte de validation: 0.316. Précision de validation: 82.77%\n",
      "Epoque57. Perte entraînement: 0.309. Perte de validation: 0.316. Précision de validation: 82.77%\n",
      "Epoque58. Perte entraînement: 0.309. Perte de validation: 0.315. Précision de validation: 82.77%\n",
      "Epoque59. Perte entraînement: 0.309. Perte de validation: 0.315. Précision de validation: 82.77%\n",
      "Epoque60. Perte entraînement: 0.309. Perte de validation: 0.315. Précision de validation: 83.00%\n",
      "Epoque61. Perte entraînement: 0.309. Perte de validation: 0.315. Précision de validation: 83.22%\n",
      "Epoque62. Perte entraînement: 0.309. Perte de validation: 0.315. Précision de validation: 83.00%\n",
      "Epoque63. Perte entraînement: 0.309. Perte de validation: 0.315. Précision de validation: 83.22%\n",
      "Epoque64. Perte entraînement: 0.308. Perte de validation: 0.314. Précision de validation: 83.22%\n",
      "Epoque65. Perte entraînement: 0.308. Perte de validation: 0.314. Précision de validation: 83.22%\n",
      "Epoque66. Perte entraînement: 0.308. Perte de validation: 0.314. Précision de validation: 83.22%\n",
      "Epoque67. Perte entraînement: 0.308. Perte de validation: 0.314. Précision de validation: 83.22%\n",
      "Epoque68. Perte entraînement: 0.308. Perte de validation: 0.314. Précision de validation: 83.00%\n",
      "Epoque69. Perte entraînement: 0.308. Perte de validation: 0.314. Précision de validation: 83.00%\n",
      "Epoque70. Perte entraînement: 0.308. Perte de validation: 0.314. Précision de validation: 82.77%\n",
      "Epoque71. Perte entraînement: 0.307. Perte de validation: 0.314. Précision de validation: 82.55%\n",
      "Epoque72. Perte entraînement: 0.307. Perte de validation: 0.313. Précision de validation: 82.77%\n",
      "Epoque73. Perte entraînement: 0.307. Perte de validation: 0.313. Précision de validation: 82.77%\n",
      "Epoque74. Perte entraînement: 0.307. Perte de validation: 0.313. Précision de validation: 82.77%\n",
      "Epoque75. Perte entraînement: 0.307. Perte de validation: 0.313. Précision de validation: 82.55%\n",
      "Epoque76. Perte entraînement: 0.307. Perte de validation: 0.313. Précision de validation: 82.55%\n",
      "Epoque77. Perte entraînement: 0.307. Perte de validation: 0.313. Précision de validation: 82.55%\n",
      "Epoque78. Perte entraînement: 0.307. Perte de validation: 0.313. Précision de validation: 82.55%\n",
      "Epoque79. Perte entraînement: 0.306. Perte de validation: 0.313. Précision de validation: 82.55%\n",
      "Epoque80. Perte entraînement: 0.306. Perte de validation: 0.313. Précision de validation: 82.55%\n",
      "Epoque81. Perte entraînement: 0.306. Perte de validation: 0.312. Précision de validation: 82.77%\n",
      "Epoque82. Perte entraînement: 0.306. Perte de validation: 0.312. Précision de validation: 82.77%\n",
      "Epoque83. Perte entraînement: 0.306. Perte de validation: 0.312. Précision de validation: 82.55%\n",
      "Epoque84. Perte entraînement: 0.306. Perte de validation: 0.312. Précision de validation: 82.55%\n",
      "Epoque85. Perte entraînement: 0.306. Perte de validation: 0.312. Précision de validation: 82.77%\n",
      "Epoque86. Perte entraînement: 0.306. Perte de validation: 0.312. Précision de validation: 82.55%\n",
      "Epoque87. Perte entraînement: 0.306. Perte de validation: 0.312. Précision de validation: 82.55%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoque88. Perte entraînement: 0.306. Perte de validation: 0.312. Précision de validation: 82.55%\n",
      "Epoque89. Perte entraînement: 0.305. Perte de validation: 0.312. Précision de validation: 82.77%\n",
      "Epoque90. Perte entraînement: 0.305. Perte de validation: 0.312. Précision de validation: 82.77%\n",
      "Epoque91. Perte entraînement: 0.305. Perte de validation: 0.311. Précision de validation: 82.77%\n",
      "Epoque92. Perte entraînement: 0.305. Perte de validation: 0.311. Précision de validation: 83.00%\n",
      "Epoque93. Perte entraînement: 0.305. Perte de validation: 0.311. Précision de validation: 83.00%\n",
      "Epoque94. Perte entraînement: 0.305. Perte de validation: 0.311. Précision de validation: 83.00%\n",
      "Epoque95. Perte entraînement: 0.305. Perte de validation: 0.311. Précision de validation: 83.00%\n",
      "Epoque96. Perte entraînement: 0.305. Perte de validation: 0.311. Précision de validation: 83.00%\n",
      "Epoque97. Perte entraînement: 0.305. Perte de validation: 0.311. Précision de validation: 83.00%\n",
      "Fin de l'entrainement.\n"
     ]
    }
   ],
   "source": [
    "# Choisir la taille du lot\n",
    "batch_size = 100\n",
    "\n",
    "# les points d'arret de l'algorithme\n",
    "max_epochs = 100\n",
    "prev_validation_loss = 999999999.\n",
    "\n",
    "# Charger le premier lot de formation et de validation, en utilisant la classe que nous avons créée. \n",
    "# Les arguments se terminent par'Audiobooks_Data_<...>', où pour <...> on entre'train','validation' ou'test'.\n",
    "train_data = Audiobooks_Data_Reader('train', batch_size)\n",
    "validation_data = Audiobooks_Data_Reader('validation')\n",
    "\n",
    "# créer une boucle pour les époques\n",
    "for epoch_counter in range(max_epochs) :\n",
    "    \n",
    "    # la mise de l'époque loss en 0.\n",
    "    curr_epoch_loss = 0.\n",
    "    \n",
    "    # Itérer sur les données d'entraînement \n",
    "    # Puisque train_data est une instance de la classe Audiobooks_Data_Reader,\n",
    "    # nous pouvons itérer à travers elle en utilisant implicitement la méthode __next__ que nous avons définie ci-dessus.\n",
    "    # Pour rappel, il regroupe les échantillons en lots, code les cibles en une seule fois et renvoie les résultats.\n",
    "    # Entrées et cibles lot par lot Nombre d'entrées et de cibles lot par lot\n",
    "    for input_batch, target_batch in train_data :\n",
    "        _,batch_loss = sess.run([optimizer, mean_loss], \n",
    "                       feed_dict={inputs : input_batch, targets : target_batch})\n",
    "        \n",
    "        #Enregistrer la perte de lot dans la perte d'époque actuelle.\n",
    "        curr_epoch_loss += batch_loss\n",
    "    \n",
    "    # trouver le moyen curr_epoch_loss.\n",
    "    # batch_count est une variable, définie dans la classe Audiobooks_Data_Reader\n",
    "    curr_epoch_loss /= train_data.batch_count\n",
    "    \n",
    "    # Définir la perte de validation et la précision de l'époque à zéro\n",
    "    validation_loss = 0.\n",
    "    validation_accuracy = 0.\n",
    "    \n",
    "    # Utiliser la même logique que le code pour propager l'ensemble de validation\n",
    "    # Il n'y aura qu'un seul lot, car la classe a été créée de cette façon\n",
    "    for input_batch, target_batch in validation_data :\n",
    "        validation_loss, validation_accuracy = sess.run([mean_loss, accuracy],\n",
    "                                               feed_dict={inputs : input_batch, targets : target_batch})\n",
    "    \n",
    "    # Imprimer les statistiques pour l'époque actuelle\n",
    "    print('Epoque'+str(epoch_counter+1)+\n",
    "          '. Perte entraînement: '+'{0:.3f}'.format(curr_epoch_loss)+\n",
    "          '. Perte de validation: '+'{0:.3f}'.format(validation_loss)+\n",
    "          '. Précision de validation: '+'{0:.2f}'.format(validation_accuracy  * 100.)+'%')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Déclenchement de l'arrêt anticipé si la perte de validation commence à augmenter.\n",
    "    if validation_loss > prev_validation_loss :\n",
    "        \n",
    "          break\n",
    "        \n",
    "    # Mémoriser la perte de validation de cette époque pour l'utiliser comme précédemment lors de la prochaine itération.\n",
    "    prev_validation_loss = validation_loss \n",
    "    \n",
    "print(\"Fin de l'entrainement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Le Test du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 80.36%\n"
     ]
    }
   ],
   "source": [
    "# Charger les données de test, en suivant la même logique que pour les données train_data et les données de validation.\n",
    "test_data = Audiobooks_Data_Reader('test')\n",
    "\n",
    "# Se propager vers l'avant à travers l'ensemble d'entraînement. Cette fois, nous n'avons besoin que de la précision\n",
    "\n",
    "for inputs_batch, targets_batch in test_data:\n",
    "    test_accuracy = sess.run([accuracy],\n",
    "                     feed_dict={inputs: inputs_batch, targets: targets_batch})\n",
    "\n",
    "#obtenir la précision du test en pourcentage\n",
    "# Quand sess.run a une seule sortie, nous obtenons une liste (c'est ainsi qu'elle a été codée par Google), plutôt qu'un flotteur.\n",
    "# Par conséquent, nous devons prendre la première valeur de la liste (la valeur à la position 0)\n",
    "test_accuracy_percent = test_accuracy[0] * 100.\n",
    "\n",
    "# afficher la précision du test\n",
    "print('Test accuracy: '+'{0:.2f}'.format(test_accuracy_percent)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La précision du modèle est de 80 % donc si on prend par exemple 100 clients qui ont achetés les livres audio le modèle peut prédire le comportement d'achat exactes des 80 clients(80 %)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Décision Stratégique :** Comme une solution stratégique de la société AudioBook, elle doit se concentrer ces clients en leurs données des offres spéciales par exemple des réductions sur les achats(coupons), des cartes de fidélités car ils représentent le potentiel de d'achat pour AudioBook et c'est sûr qu'ils vont acheter les produits à nouveaux."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
